{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Challenge 7 - Fight Fire with Data\n## Random Forest Model to Predict Fire Spread\n\nThe user will be using a Jupyter Notebook to run code that was developed in Python. First, the user will check to see if the wind speed and brightness are correlated with the speed that the fire spreads derived from the satellite data. The input data has been prepared for you. Next, the user will run the code that creates a model (random forest) using the features they select (windspeed and brightness) as the inputs and estimates the speed of spread as the target variable (speed of spread). They will train a model, record the Mean Absolute Error and save the model into a deployable format also known as Predictive Model Markup Language (PMML). "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Install and Load Packages"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import pandas as pd\nimport numpy as np\n# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\nimport types"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Get and View Data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df = pd.read_csv(\"Challenge_7_Merged_Data_single_fire.csv\" , low_memory=False)\ndf.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(f'Dataframe shape: {df.shape}\\n')\nprint(f'Columns: {df.columns}')\ndf.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "features_short = [\n       'WindSpeedMph', \n       'SurfaceWindGustsMph',\n       'ZeroToTenLiquidSoilMoisturePercent',\n       'TenToFortyLiquidSoilMoisturePercent',\n       'FortyToOneHundredLiquidSoilMoisturePercent',\n       'SurfaceTemperatureFahrenheit', \n       'SurfaceDewpointTemperatureFahrenheit',\n       'SurfaceWetBulbTemperatureFahrenheit', \n       'RelativeHumidityPercent',\n       'brightness', \n       'bright_t31', \n       'frp', \n       'speed_mph'] \n\n# preview our df\nprint('Display df')\ndisplay(df[features_short].head())\n\n# look at statistics of df\nprint('Describe dataframe')\ndisplay(df[features_short].describe())"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# choose features\ninput_features = features_short[:-1]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## These are the features that we will put in the model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "input_features = [\n       'WindSpeedMph', \n#        'SurfaceWindGustsMph',\n#        'ZeroToTenLiquidSoilMoisturePercent',\n#        'TenToFortyLiquidSoilMoisturePercent',\n#        'FortyToOneHundredLiquidSoilMoisturePercent',\n#        'SurfaceTemperatureFahrenheit', \n#        'SurfaceDewpointTemperatureFahrenheit',\n#        'SurfaceWetBulbTemperatureFahrenheit', \n       'RelativeHumidityPercent',\n       'brightness', \n       'bright_t31', \n       'frp' \n]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "y = np.array(df['speed_mph'])\nX = np.array(df[input_features])\nprint(y.shape)\nprint(X.shape)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Make a train/test split for the model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# make train test split\ntrain_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size = 0.25, random_state = 137)\ntrain_features.shape\ntest_features.shape\ntrain_labels.shape\ntest_labels.shape\n\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Train and test a random forest model using Kfold Validation\nHere we're going to split our training data into three-folds. For each round, two folds will be used for training, and one fold will be used for validation. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "import time\nfrom sklearn.model_selection import KFold\n\n# Instantiate model with 100 decision trees with a depth of 2\nrf = RandomForestRegressor(\n    n_estimators = 100,\n    max_depth = 2,\n    n_jobs= -1, \n    random_state = 137,\n    verbose=1\n    )\n\n# Set up cross validation\nkf = KFold(n_splits=3, shuffle=True, random_state=8)\n\n# Track start time\nstart_time = time.time()\n# Keep track of MAE for each fit\nall_mae = []\nfor train_index, test_index in kf.split(train_features):\n    X_train, X_test = train_features[train_index], train_features[test_index]\n    y_train, y_test = train_labels[train_index], train_labels[test_index]\n    \n    rf.fit(X_train, y_train)\n    predictions = rf.predict(X_test)\n    errors = (abs(predictions - y_test))\n    mae = np.mean(errors)\n    all_mae.append(mae)\n    \nprint(\"--- %s seconds ---\" % (time.time() - start_time))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Display accuracy of the model\nLet's check on how the model did on the training data. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "(f'Average Random Forest Mean Absolute Error over three folds: {np.mean(all_mae)}') # THIS IS OUR VERIFICATION CODE(0.034933)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "MAE: is it the same as yours?  \nMean Absolute Error: 0.0034933 mph.\n\n## Export Predictive Model Markup Language File.\n\nAlthough we haven't officialy tested the model on the test data, lets save it. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "https://collaborate.pega.com/discussion/creating-pmml-python-r-and-pega"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn2pmml import sklearn2pmml\nfrom sklearn2pmml.pipeline import PMMLPipeline\n\n# instantiate PMMLPipeline object\npipeline = PMMLPipeline([\n        ('random_forest', rf)])\n\n# train\npipeline.fit(train_features, train_labels)\n\n# save\nsklearn2pmml(pipeline, \"randomforest.pmml\", with_repr = True)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Import the lib\nfrom project_lib import Project\nproject = Project(sc,\"<ProjectId>\", \"<ProjectToken>\")\n\n# let's assume you have the pandas DataFrame  pandas_df which contains the data\n# you want to save in your object storage as a csv file\nproject.save_data(\"file_name.csv\", pandas_df.to_csv(index=False))\n\n# the function returns a dict which contains the asset_id, bucket_name and file_name\n# upon successful saving of the data"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Complete Challenge"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Verification \n\nimport ww\nww = ww.WatsonWarriors()\n \nww.answer(0, np.mean(all_mae))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "## Paste validation code below.\n"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}